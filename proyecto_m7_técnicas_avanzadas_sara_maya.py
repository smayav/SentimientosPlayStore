# -*- coding: utf-8 -*-
"""Proyecto M7. Técnicas avanzadas_Sara_Maya

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18cikFX-lMsj1uwb1sow2Cmb48sy5IWXF

## **Bootcamp: Ciencia de Datos e Inteligencia Artificial**
## **Proyecto del Módulo 7: Técnicas avanzadas para ciencia de datos y empleabilidad**

Hola, ya es el último proyecto, has avanzado y aprendido mucho hasta acá. ¡Muchas felicidades!

Es hora de poner en práctica todo lo que hemos aprendido a lo largo de nuestra travesía.

Lee el proyecto y revisa con cuidado cada una de las instrucciones. Procura plasmar todo tu potencial para que lo concluyas de manera sobresaliente.

¡Éxito!

# Objetivos
- Aplicar con éxito todos los conocimientos que has adquirido a lo largo del Bootcamp.
- Consolidar las técnicas de limpieza, entrenamiento, graficación y ajuste a modelos de *Machine Learning*.
- Generar una API que brinde predicciones como resultado a partir de datos enviados.

# Proyecto

1. Selecciona uno de los siguientes *datasets*:
  - *Reviews* de aplicaciones de la Google Play Store: https://www.kaggle.com/datasets/lava18/google-play-store-apps
  - Estadísticas demográficas de los ganadores del premio Oscar de la Academia: https://www.kaggle.com/datasets/fmejia21/demographics-of-academy-awards-oscars-winners
  - Aspiraciones profesionales de la generación Z: https://www.kaggle.com/datasets/kulturehire/understanding-career-aspirations-of-genz

Cada uno representa un *dataset*, un problema y una forma diferente de abordarlo. Tu tarea es identificar las técnicas y modelos que podrías usar para tu proyecto.

2. Debes hacer un análisis exploratorio y limpieza de los datos. Usa las ténicas que creas convenientes.

3. Entrena el modelo de *Machine Learning*, procesamiento de lenguaje natural o red neuronal que creas adecuado.

4. Genera por lo menos dos gráficas y dos métricas de rendimiento; explica las puntuaciones de rendimiento que amerite tu problema. Todas las gráficas de rendimiento que realices deben tener leyendas, colores y títulos personalizados por ti.

  - Además, antes de subir el modelo a "producción", deberás realizar un proceso de ensambles (*ensemblings*) y de ajuste de hiperparámetros o *tuning* para intentar mejorar la precisión y disminuir la varianza de tu modelo.

5. Construye una API REST en la que cualquier usuario pueda mandar datos y que esta misma devuelva la predicción del modelo que has hecho. La API debe estar en la nube, ya sea en un servicio como Netlify o Ngrok, para que pueda ser consultada desde internet.

6. Genera una presentación del problema y del modelo de solución que planteas. Muestra gráficas, datos de rendimiento y explicaciones. Esta presentación debe estar enfocada a personas que no sepan mucho de ciencia de datos e inteligencia artificial.

7. **Solamente se recibirán trabajos subidos a tu cuenta de GitHub con un README.md apropiado que explique tu proyecto**.

## Criterios de evaluación

| Actividad | Porcentaje | Observaciones | Punto parcial
| -- | -- | -- | -- |
| Actividad 1. Limpieza y EDA | 20 | Realiza todas las tareas necesarias para hacer el EDA y la limpieza correcta, dependiendo de la problemática. Debes hacer como mínimo el análisis de completitud, escalamiento (si aplica) y tokenización (si aplica). | Realizaste solo algunas tareas de exploración y limpieza y el modelo se muestra aún con oportunidad de completitud, escalamiento y/o mejora. |
| Actividad 2. Entrenamiento del modelo | 20 | Elige el modelo y algoritmo adecuados para tu problema, entrénalo con los datos ya limpios y genera algunas predicciones de prueba. | No has realizado predicciones de prueba para tu modelo de ML y/o tu modelo muestra una precisión menor al 60 %. |
| Actividad 3. Graficación y métricas | 20 | Genera por lo menos dos gráficas y dos muestras de métricas que permitan visualizar el rendimiento y precisión del modelo que construiste. Además, realizaste los procesos de *tuning* y ensambles adecuados para tu problema. | Las gráficas no tienen leyendas y colores customizados, solo muestras una gráfica o no realizaste el *tuning* de hiperparámetros.
| Actividad 4. API REST | 20 | Generaste con éxito un *link* público en el que, por método POST, se puede mandar información y la API REST devuelve una predicción junto con el porcentaje de confianza de esta misma. | N/A
| Actividad 5. Presentación | 20 | Genera una presentación en la que establezcas como mínimo: el problema, proceso de solución, metodologías usadas, gráficas de rendimiento, demostración del modelo y aprendizajes obtenidos. Debes redactarla con términos que pueda entender cualquier persona, no solo científicos de datos. | La presentación no expone con claridad o en términos coloquiales el proceso de creación del modelo, sus ventajas y muestras de rendimiento.

**Mucho éxito en tu camino como Data Scientist.**
"""


#Librerías
import pandas as pd
import numpy as np
import json
import csv
import random

import joblib
import matplotlib.pyplot as plt
import seaborn as sns

import re
import string
import unidecode

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer

from wordcloud import WordCloud

from sklearn.feature_extraction.text import (
    TfidfVectorizer,
    CountVectorizer,
)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    classification_report,
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier

# Descargar recursos necesarios desde NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Importar datos
df1 = pd.read_csv('googleplaystore.csv')

# Importar datos
df2 = pd.read_csv('googleplaystore_user_reviews.csv')

df1.head()

df1.info()

df2.head()

df2.info()

"""Eliminar los nan de la columna Sentiment"""

df = df2.dropna(subset=['Translated_Review'])

df = df.dropna(subset=['Sentiment'])

df = df.dropna(subset=['App'])

df = df[['App', 'Translated_Review', 'Sentiment']]

df.head()

df.info()

# Graficar un gráfico de torta
counts = df['Sentiment'].value_counts()

plt.pie(counts.values, labels=counts.index, colors=["turquoise", "lightgreen", "lightcoral"], autopct='%1.1f%%')
plt.title("Distribution of sentiment")
plt.savefig('Distribution_of_sentiment.png', bbox_inches='tight')
plt.show()

stop_words_en = set(stopwords.words('english'))

extra_stopwords = {'quot', 'lol', 'amp', 'today'}
stop_words_en.update(extra_stopwords)

def clean_text(text):
  # Convertimos a minúsculas
  text = text.lower()

  # Removemos tags HTML
  text = re.sub(re.compile('<.*?>'), '', text)

  # Tomamos solo las palabras
  text = re.sub('[^A-Za-z0-9]+', ' ', text)

  #Remover puntuación
  #text = re.sub('[^a-zA-Z]', ' ', text)
  text = text.translate(str.maketrans('', '', string.punctuation))

  #Remover símbolos
  text=re.sub("&lt;/?.*?&gt;"," &lt;&gt; ",text)

  #Remover dígitos y carácteres especiales
  text=re.sub("(\\d|\\W)+"," ",text)
  #texto_limpio.append(desc)

  #Quitar acentos
  text = unidecode.unidecode(text)

  # Tokenización
  tokens = nltk.word_tokenize(text)

  # Removemos las palabras de parada
  text = [word for word in tokens if word not in stop_words_en]

  # Unimos las palabras
  text = ' '.join(text)

  return text

df["Texto_Limpio"] = df['Translated_Review'].apply(clean_text)
df.head()

# Combinar todos los textos limpios en una sola cadena

text_combined = ' '.join(df["Texto_Limpio"].astype(str))

# Crear la nube de palabras
wordcloud = WordCloud(stopwords=stop_words_en).generate(text_combined)

# Mostrar la nube de palabras usando Matplotlib
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud")
plt.show()

"""## Clasificación de reviews


"""

# Definir positivos y negativos
positivo = df[df['Sentiment'] == 'Positive']
negativo = df[df['Sentiment'] == 'Negative']

# Nube de palabras de comentarios positivos

# Concatenar todos los textos limpios de comentarios positivos en una sola cadena
positivo_text_combined = ' '.join(review for review in positivo['Texto_Limpio'].astype(str))

# Generar la nube de palabras para comentarios positivos

wordcloud_positivo = WordCloud(stopwords=stop_words_en).generate(positivo_text_combined)

# Mostrar la nube de palabras para comentarios positivos
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud_positivo, interpolation='bilinear')
plt.axis('off')
plt.title("Nube de palabras positivo")
wordcloud_positivo.to_file('nube_palabras_positivo.png')
#plt.show()

# Nube de palabras de comentarios negativos
negativo_text_combined = ' '.join(review for review in negativo['Texto_Limpio'].astype(str))

# Generar la nube de palabras para comentarios negativos
#wordcloud_negativo = WordCloud(max_words=100, stopwords=STOPWORDS, background_color='white').generate(negativo_text_combined)
wordcloud_negativo = WordCloud(stopwords=stop_words_en).generate(negativo_text_combined)

# Mostrar la nube de palabras para comentarios negativos
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud_negativo, interpolation='bilinear')
plt.axis('off')
plt.title("Nube de palabras comentarios negativos")
wordcloud_negativo.to_file('nube_palabras_negativo.png')
#plt.show()

"""## Construcción del modelo"""

# Construcción del DataFrame para el modelo
dfNew = df[['Texto_Limpio','Sentiment']]
dfNew.head()

index = dfNew.index
index

#dfNew['random_number'] = np.random.randn(len(index))

random_numbers = np.random.randn(len(index))
dfNew['random_number'] = random_numbers

dfNew["random_number"]

train = dfNew[dfNew['random_number'] <= 0.8]
test = dfNew[dfNew['random_number'] > 0.8]

train.shape, test.shape

# Vectorización de texto utilizando CountVectorizer
#Creación de la matriz de características para el conjunto de entrenamiento y prueba
vectorizer = CountVectorizer(token_pattern=r'\b\w+\b')
train_matrix = vectorizer.fit_transform(train['Texto_Limpio'])
test_matrix = vectorizer.transform(test['Texto_Limpio'])
joblib.dump(vectorizer,"vectorizer.pkl")

# Información sobre la matriz de características del conjunto de entrenamiento
train_matrix

# Información sobre la matriz de características del conjunto de prueba
test_matrix

lr = LogisticRegression(max_iter=1000)

# Asignación de las matrices de características y las etiquetas para entrenamiento y prueba
X_train = train_matrix
X_test = test_matrix
y_train = train['Sentiment']
y_test = test['Sentiment']

# Entrenamiento del modelo de Regresión Logística
lr.fit(X_train, y_train)

# Predicción utilizando el modelo entrenado en el conjunto de prueba
predictions = lr.predict(X_test)
print(X_test)

# Matriz de confusión
new = np.asarray(y_test)
cf_matrix = confusion_matrix(predictions,y_test)
cf_matrix

# # Visualización de la matriz de confusión utilizando un mapa de calor
# sns.heatmap(cf_matrix, annot=True)
# plt.show()

class_labels = ['Negative', 'Neutral', 'Positive']

plt.figure(figsize=(8, 6))
sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Clases Predichas')
plt.ylabel('Clases Verdaderas')
plt.title('Matriz de Confusión')
plt.savefig('matriz_confusion.png', bbox_inches='tight')
plt.show()

# Reporte de clasificación con métricas de precisión, recall y f1-score
print(classification_report(predictions,y_test))

joblib.dump(lr, "modelo.pkl")

# model2 = BaggingClassifier(
#     estimator=KNeighborsClassifier(),
#     n_estimators=500,
#     random_state=19
# ).fit(X_train, y_train)

# model2_predict = model2.predict(X_test)
# print('El Accuracy usando Bagging con KNN es de: {}'.format(accuracy_score(model2_predict, y_test)))

# Prueba de predicción con una nueva revisión ('review_test')
review_test = "I use this app"


# Debemos hacer que nuestro texto de test pase por el mismo proceso que todo el dataset de train
review_convert = vectorizer.transform([review_test])
prediction_test = lr.predict(review_convert)
prediction_test

# Lista de varios tweets a evaluar
tweets_to_evaluate = [
    "Great app! Very useful for daily tasks.",
    "The latest update ruined the experience, constant crashes.",
    "Love the new features, keep up the good work!",
    "I can't even open the app after the recent update.",
    "The app design is sleek and easy to navigate.",
    "Useless app, doesn't do what it promises.",
    "Best app for productivity, highly recommended.",
    "Annoying ads keep popping up, making it unusable.",
    "The app froze and I lost all my progress, very frustrating.",
    "Simple and efficient, exactly what I needed."

]

# Transformar los tweets usando el vectorizador
converted_tweets = vectorizer.transform(tweets_to_evaluate)

# Predecir los sentimientos de los tweets
predictions = lr.predict(converted_tweets)
print(predictions)